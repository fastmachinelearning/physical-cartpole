
Number of samples in training set: 718080
The mean number of samples from each experiment used for training is 17952.0 with variance 0.0
Number of samples in validation set: 89760

350/350 [==============================] - 8s 22ms/step - loss: 52.7727

Validation loss before starting training is 52.7727165222168
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 layers_0 (QDense)           (None, 50, 32)            256       
                                                                 
 q_activation (QActivation)  (None, 50, 32)            0         
                                                                 
 layers_1 (QDense)           (None, 50, 32)            1056      
                                                                 
 q_activation_1 (QActivatio  (None, 50, 32)            0         
 n)                                                              
                                                                 
 layers_2 (QDense)           (None, 50, 1)             33        
                                                                 
=================================================================
Total params: 1345 (5.25 KB)
Trainable params: 1345 (5.25 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
Epoch 1/50
2805/2805 [==============================] - 146s 52ms/step - loss: 2.5016 - val_loss: 1.5084 - lr: 0.0200

Epoch 2/50
2805/2805 [==============================] - 144s 51ms/step - loss: 1.3422 - val_loss: 1.2462 - lr: 0.0200

Epoch 3/50
2805/2805 [==============================] - 144s 51ms/step - loss: 1.1956 - val_loss: 1.1239 - lr: 0.0200

Epoch 4/50
2805/2805 [==============================] - 214s 76ms/step - loss: 1.1272 - val_loss: 1.1022 - lr: 0.0200

Epoch 5/50
2805/2805 [==============================] - 139s 50ms/step - loss: 1.0941 - val_loss: 1.0704 - lr: 0.0200

Epoch 6/50
2805/2805 [==============================] - 138s 49ms/step - loss: 1.0476 - val_loss: 0.9885 - lr: 0.0200

Epoch 7/50
2805/2805 [==============================] - 143s 51ms/step - loss: 1.0240 - val_loss: 1.0588 - lr: 0.0200

Epoch 8/50
2805/2805 [==============================] - 147s 52ms/step - loss: 1.0046 - val_loss: 0.9874 - lr: 0.0200

Epoch 9/50
2805/2805 [==============================] - 147s 52ms/step - loss: 1.0015 - val_loss: 1.0049 - lr: 0.0200

Epoch 10/50
2805/2805 [==============================] - 145s 52ms/step - loss: 0.9881 - val_loss: 0.9823 - lr: 0.0200

Epoch 11/50
2805/2805 [==============================] - 162s 58ms/step - loss: 0.9860 - val_loss: 1.0108 - lr: 0.0200

Epoch 12/50
2805/2805 [==============================] - ETA: 0s - loss: 0.9746

Epoch 12: ReduceLROnPlateau reducing learning rate to 0.009999999776482582.
2805/2805 [==============================] - 255s 91ms/step - loss: 0.9746 - val_loss: 1.0215 - lr: 0.0200

Epoch 13/50
2805/2805 [==============================] - 161s 57ms/step - loss: 0.8966 - val_loss: 0.9080 - lr: 0.0100

Epoch 14/50
2805/2805 [==============================] - 142s 51ms/step - loss: 0.8971 - val_loss: 0.9655 - lr: 0.0100

Epoch 15/50
2803/2805 [============================>.] - ETA: 0s - loss: 0.8923

Epoch 15: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.
2805/2805 [==============================] - 142s 51ms/step - loss: 0.8923 - val_loss: 0.9112 - lr: 0.0100

Epoch 16/50
2805/2805 [==============================] - 144s 51ms/step - loss: 0.8603 - val_loss: 0.8961 - lr: 0.0050

Epoch 17/50
2805/2805 [==============================] - 142s 51ms/step - loss: 0.8578 - val_loss: 0.8807 - lr: 0.0050

Epoch 18/50
2805/2805 [==============================] - 141s 50ms/step - loss: 0.8579 - val_loss: 0.8978 - lr: 0.0050

Epoch 19/50
2803/2805 [============================>.] - ETA: 0s - loss: 0.8541

Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.
2805/2805 [==============================] - 142s 51ms/step - loss: 0.8541 - val_loss: 0.9014 - lr: 0.0050

Epoch 20/50
2805/2805 [==============================] - 142s 51ms/step - loss: 0.8377 - val_loss: 0.8704 - lr: 0.0025

Epoch 21/50
2805/2805 [==============================] - 143s 51ms/step - loss: 0.8393 - val_loss: 0.8919 - lr: 0.0025

Epoch 22/50
2805/2805 [==============================] - 144s 51ms/step - loss: 0.8374 - val_loss: 0.8665 - lr: 0.0025

Epoch 23/50
2805/2805 [==============================] - 162s 58ms/step - loss: 0.8372 - val_loss: 0.8861 - lr: 0.0025

Epoch 24/50
2805/2805 [==============================] - ETA: 0s - loss: 0.8375

Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.
2805/2805 [==============================] - 141s 50ms/step - loss: 0.8375 - val_loss: 0.8729 - lr: 0.0025

Epoch 25/50
2805/2805 [==============================] - 141s 50ms/step - loss: 0.8258 - val_loss: 0.8613 - lr: 0.0012

Epoch 26/50
2805/2805 [==============================] - 145s 52ms/step - loss: 0.8271 - val_loss: 0.8735 - lr: 0.0012

Epoch 27/50
2805/2805 [==============================] - ETA: 0s - loss: 0.8264

Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.
2805/2805 [==============================] - 145s 52ms/step - loss: 0.8264 - val_loss: 0.8638 - lr: 0.0012

Epoch 28/50
2805/2805 [==============================] - 145s 52ms/step - loss: 0.8221 - val_loss: 0.8599 - lr: 6.2500e-04

Epoch 29/50
2805/2805 [==============================] - 144s 51ms/step - loss: 0.8213 - val_loss: 0.8678 - lr: 6.2500e-04

Epoch 30/50
2803/2805 [============================>.] - ETA: 0s - loss: 0.8209

Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.
2805/2805 [==============================] - 142s 51ms/step - loss: 0.8208 - val_loss: 0.8682 - lr: 6.2500e-04

Epoch 31/50
2805/2805 [==============================] - 140s 50ms/step - loss: 0.8191 - val_loss: 0.8576 - lr: 3.1250e-04

Epoch 32/50
2805/2805 [==============================] - 141s 50ms/step - loss: 0.8191 - val_loss: 0.8728 - lr: 3.1250e-04

Epoch 33/50
2805/2805 [==============================] - 143s 51ms/step - loss: 0.8185 - val_loss: 0.8562 - lr: 3.1250e-04

Epoch 34/50
2805/2805 [==============================] - 143s 51ms/step - loss: 0.8190 - val_loss: 0.8640 - lr: 3.1250e-04

Epoch 35/50
2804/2805 [============================>.] - ETA: 0s - loss: 0.8193

Epoch 35: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.
2805/2805 [==============================] - 167s 60ms/step - loss: 0.8193 - val_loss: 0.8689 - lr: 3.1250e-04

Epoch 36/50
2805/2805 [==============================] - 145s 52ms/step - loss: 0.8179 - val_loss: 0.8553 - lr: 1.5625e-04

Epoch 37/50
2805/2805 [==============================] - 223s 80ms/step - loss: 0.8180 - val_loss: 0.8563 - lr: 1.5625e-04

Epoch 38/50
2805/2805 [==============================] - 235s 84ms/step - loss: 0.8169 - val_loss: 0.8520 - lr: 1.5625e-04

Epoch 39/50
2805/2805 [==============================] - 145s 52ms/step - loss: 0.8164 - val_loss: 0.8545 - lr: 1.5625e-04

Epoch 40/50
2805/2805 [==============================] - ETA: 0s - loss: 0.8165

Epoch 40: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.
2805/2805 [==============================] - 146s 52ms/step - loss: 0.8165 - val_loss: 0.8585 - lr: 1.5625e-04

Epoch 41/50
2805/2805 [==============================] - 144s 51ms/step - loss: 0.8160 - val_loss: 0.8536 - lr: 7.8125e-05

Epoch 42/50
2803/2805 [============================>.] - ETA: 0s - loss: 0.8153

Epoch 42: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.
2805/2805 [==============================] - 141s 50ms/step - loss: 0.8153 - val_loss: 0.8563 - lr: 7.8125e-05

Epoch 43/50
2805/2805 [==============================] - 144s 52ms/step - loss: 0.8152 - val_loss: 0.8550 - lr: 3.9062e-05

Epoch 44/50
2802/2805 [============================>.] - ETA: 0s - loss: 0.8157

Epoch 44: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.
2805/2805 [==============================] - 162s 58ms/step - loss: 0.8157 - val_loss: 0.8578 - lr: 3.9062e-05

Epoch 45/50
2805/2805 [==============================] - 143s 51ms/step - loss: 0.8150 - val_loss: 0.8586 - lr: 1.9531e-05

Epoch 46/50
2805/2805 [==============================] - ETA: 0s - loss: 0.8150

Epoch 46: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.
2805/2805 [==============================] - 142s 51ms/step - loss: 0.8150 - val_loss: 0.8534 - lr: 1.9531e-05

Epoch 47/50
2805/2805 [==============================] - 143s 51ms/step - loss: 0.8144 - val_loss: 0.8576 - lr: 9.7656e-06

Epoch 48/50
2803/2805 [============================>.] - ETA: 0s - loss: 0.8146

Epoch 48: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.
2805/2805 [==============================] - 146s 52ms/step - loss: 0.8145 - val_loss: 0.8619 - lr: 9.7656e-06

Epoch 49/50
2805/2805 [==============================] - 146s 52ms/step - loss: 0.8146 - val_loss: 0.8556 - lr: 4.8828e-06

Epoch 50/50
2804/2805 [============================>.] - ETA: 0s - loss: 0.8149

Epoch 50: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.
2805/2805 [==============================] - 146s 52ms/step - loss: 0.8149 - val_loss: 0.8535 - lr: 4.8828e-06

... quantizing model
Interpreting Sequential
Topology:
Layer name: input_1, layer type: InputLayer, input shapes: [[None, 50, 7]], output shape: [None, 50, 7]
Layer name: layers_0, layer type: QDense, input shapes: [[None, 50, 7]], output shape: [None, 50, 32]
Layer name: q_activation, layer type: Activation, input shapes: [[None, 50, 32]], output shape: [None, 50, 32]
Layer name: layers_1, layer type: QDense, input shapes: [[None, 50, 32]], output shape: [None, 50, 32]
Layer name: q_activation_1, layer type: Activation, input shapes: [[None, 50, 32]], output shape: [None, 50, 32]
Layer name: layers_2, layer type: QDense, input shapes: [[None, 50, 32]], output shape: [None, 50, 1]
Model
  Precision:         ap_fixed<18,6>
  ReuseFactor:       32
  Strategy:          Resources
  BramFactor:        1000000000
  TraceOutput:       False
LayerName
  input_1
    Trace:           False
    Precision
      result:        ap_fixed<12,2>
      weight:        ap_fixed<14,4>
      bias:          ap_fixed<14,4>
  layers_0
    Trace:           False
    Precision
      result:        ap_fixed<18,6>
      weight:        ap_fixed<14,4>
      bias:          ap_fixed<14,4>
  layers_0_quantized_bits
    Trace:           False
    Precision
      result:        ap_fixed<18,6>
      weight:        ap_fixed<14,4>
      bias:          ap_fixed<14,4>
  q_activation
    Trace:           False
    Precision
      result:        ap_fixed<12,1>
  layers_1
    Trace:           False
    Precision
      result:        ap_fixed<18,6>
      weight:        ap_fixed<14,4>
      bias:          ap_fixed<14,4>
  layers_1_quantized_bits
    Trace:           False
    Precision
      result:        ap_fixed<18,6>
      weight:        ap_fixed<14,4>
      bias:          ap_fixed<14,4>
  q_activation_1
    Trace:           False
    Precision
      result:        ap_fixed<12,1>
  layers_2
    Trace:           False
    Precision
      result:        ap_fixed<18,6>
      weight:        ap_fixed<14,4>
      bias:          ap_fixed<14,4>
  layers_2_quantized_bits
    Trace:           False
    Precision
      result:        ap_fixed<12,2>
      weight:        ap_fixed<14,4>
      bias:          ap_fixed<14,4>
Interpreting Sequential
Topology:
Layer name: input_1, layer type: InputLayer, input shapes: [[None, 50, 7]], output shape: [None, 50, 7]
Layer name: layers_0, layer type: QDense, input shapes: [[None, 50, 7]], output shape: [None, 50, 32]
Layer name: q_activation, layer type: Activation, input shapes: [[None, 50, 32]], output shape: [None, 50, 32]
Layer name: layers_1, layer type: QDense, input shapes: [[None, 50, 32]], output shape: [None, 50, 32]
Layer name: q_activation_1, layer type: Activation, input shapes: [[None, 50, 32]], output shape: [None, 50, 32]
Layer name: layers_2, layer type: QDense, input shapes: [[None, 50, 32]], output shape: [None, 50, 1]
Creating HLS model
WARNING: Layer layers_0 requires "dataflow" pipeline style. Switching to "dataflow" pipeline style.
Writing HLS project
Done

Calculating activations statistics...
For each - except for last - layer the calculation is done twice: with and without the activation function
Training Completed...                                               
 
Total time of training the network: 7840.168098380789
